{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de Lenguaje natural en español\n",
    "\n",
    " - Las principales librerías están entrenadas con corpus en inglés, algunas aproximaciones sugieren realizar una traducción de los textos al inglés antes de procesarlos, pero todos sabemos lo que sucede cuando se traduce.\n",
    " - Otra aproximación es encontrar un corpus en español y entrenar un modelo usando una librería, eso haremos hoy con Spacy\n",
    "\n",
    "¿Dónde obtener un Corpus en español?\n",
    "Veamos el trabajo de los amigos de la Sociedad Española de Procesamiento del Lenguaje Natural y su TASS [Taller de Análisis Semántico en la SEPLN](http://tass.sepln.org)\n",
    "\n",
    "Este código ha sido adaptado al español del [NLP Tutorial 8 - Sentiment Classification using SpaCy for IMDB and Amazon Review Dataset](https://www.youtube.com/watch?v=cd51nXNpiiU) creado por [Aarya Tadvalkar](https://kgptalkie.com/author/aarya/)\n",
    "\n",
    "Para que lo sigan con más facilidad está disponible en el repo de GitHb https://github.com/WomenWhoCode/WWCodeBogota/tree/master/Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar librerías\n",
    "# pip3 install scikit-learn\n",
    "# pip3 install -U spacy\n",
    "# python3 -m spacy download es\n",
    "# python3 -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hay un perro en la primera frase. Y en esta es otra una Naranja. Aquí está la frase del tercer lugar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hay un perro en la primera frase. Y en esta es otra una Naranja. Aquí está la frase del tercer lugar"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay\n",
      "un\n",
      "perro\n",
      "en\n",
      "la\n",
      "primera\n",
      "frase\n",
      ".\n",
      "Y\n",
      "en\n",
      "esta\n",
      "es\n",
      "otra\n",
      "una\n",
      "Naranja\n",
      ".\n",
      "Aquí\n",
      "está\n",
      "la\n",
      "frase\n",
      "del\n",
      "tercer\n",
      "lugar\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "orac = nlp.create_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(orac, before='parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay un perro en la primera frase.\n",
      "Y en esta es otra una Naranja.\n",
      "Aquí está la frase del tercer lugar\n"
     ]
    }
   ],
   "source": [
    "for orac in doc.sents:\n",
    "    print(orac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['todo', 'sabe', 'soyos', 'grandes', 'sino', 'parte', 'trabajar', 'cuanto', 'posible', 'estoy', 'hacerlo', 'breve', 'se', 'puedo', 'toda', 'tiempo', 'todavia', 'esta', 'ciertos', 'igual', 'siendo', 'le', 'podria', 'realizó', 'saben', 'tal', 'cuantos', 'propia', 'suyas', 'unas', 'sigue', 'ha', 'ultimo', 'tú', 'podría', 'general', 'como', 'aquello', 'eres', 'tambien', 'sabeis', 'alli', 'claro', 'aún', 'tenga', 'sabemos', 'dado', 'dio', 'vaya', 'es', 'mas', 'peor', 'saber', 'llevar', 'sola', 'cómo', 'encuentra', 'añadió', 'mía', 'ir', 'lleva', 'aqui', 'deben', 'aquéllos', 'cuál', 'me', 'nosotros', 'uso', 'dos', 'quienes', 'parece', 'mi', 'otra', 'dice', 'quizá', 'buen', 'usan', 'pues', 'cuando', 'quedó', 'usas', 'aproximadamente', 'vosotras', 'trabajan', 'dijeron', 'cuales', 'tuya', 'eso', 'emplean', 'afirmó', 'en', 'si', 'trabajamos', 'el', 'tercera', 'mias', 'habia', 'fui', 'pueda', 'tiene', 'del', 'queremos', 'poder', 'hablan', 'estar', 'vuestras', 'nuestro', 'temprano', 'estos', 'tenemos', 'estais', 'ex', 'podriamos', 'ninguna', 'consideró', 'ellas', 'aquélla', 'podrían', 'quiere', 'demasiado', 'vamos', 'mías', 'una', 'gueno', 'adelante', 'intentais', 'tener', 'nada', 'pesar', 'aquellos', 'hacia', 'nuevo', 'quiénes', 'alguna', 'haciendo', 'ver', 'tu', 'sus', 'no', 'podriais', 'conseguir', 'mucha', 'antano', 'ésos', 'bastante', 'entonces', 'intenta', 'dónde', 'pronto', 'sean', 'trabajais', 'intentan', 'ningún', 'cuantas', 'ambos', 'vuestra', 'arriba', 'allí', 'esto', 'ciertas', 'aquí', 'podrán', 'consiguen', 'detrás', 'delante', 'diferente', 'informó', 'aquellas', 'ningunos', 'buena', 'principalmente', 'consigo', 'tengo', 'poco', 'sera', 'haces', 'usar', 'otro', 'míos', 'bueno', 'tuyas', 'ejemplo', 'modo', 'al', 'muchos', 'otras', 'tarde', 'según', 'usamos', 'pasada', 'primera', 'cuánto', 'mismos', 'sí', 'te', 'cuáles', 'mismo', 'algo', 'buenas', 'nosotras', 'junto', 'hago', 'empleais', 'suyo', 'siete', 'usted', 'alguno', 'habrá', 'existen', 'cosas', 'de', 'incluso', 'momento', 'repente', 'mientras', 'alrededor', 'había', 'buenos', 'indicó', 'ahi', 'haceis', 'otros', 'solo', 'dan', 'podemos', 'últimas', 'lado', 'ti', 'nadie', 'fuimos', 'hace', 'empleo', 'tuyos', 'porque', 'qué', 'ayer', 'existe', 'hicieron', 'asi', 'hacen', 'ése', 'la', 'pudo', 'mayor', 'ocho', 'nuestras', 'pocas', 'su', 'últimos', 'están', 'informo', 'verdadero', 'muy', 'mucho', 'final', 'vosotros', 'algunas', 'he', 'ella', 'van', 'acuerdo', 'solamente', 'cuántos', 'puede', 'detras', 'quién', 'uno', 'ni', 'podrá', 'horas', 'podrias', 'adrede', 'ahí', 'quizás', 'algún', 'nuestros', 'cinco', 'os', 'conmigo', 'despues', 'menos', 'intentas', 'éstas', 'ahora', 'verdadera', 'les', 'comentó', 'casi', 'explicó', 'tres', 'también', 'consigues', 'dicho', 'tendrán', 'fin', 'está', 'solas', 'lugar', 'antes', 'nunca', 'último', 'vais', 'hacer', 'mí', 'voy', 'diferentes', 'mediante', 'trabajas', 'manifestó', 'agregó', 'soy', 'actualmente', 'hoy', 'quien', 'pueden', 'nuevos', 'ustedes', 'anterior', 'que', 'señaló', 'fueron', 'esas', 'mal', 'expresó', 'siguiente', 'vez', 'así', 'entre', 'unos', 'bajo', 'cuándo', 'tampoco', 'mia', 'debajo', 'serán', 'días', 'da', 'solos', 'estaban', 'despacio', 'sido', 'fue', 'hacemos', 'estan', 'nueva', 'cuántas', 'mejor', 'primeros', 'aquéllas', 'dentro', 'realizar', 'éstos', 'tan', 'todos', 'dar', 'largo', 'ellos', 'estado', 'conseguimos', 'debe', 'usa', 'aun', 'estaba', 'ampleamos', 'sé', 'teneis', 'vuestros', 'hemos', 'respecto', 'haber', 'apenas', 'desde', 'propios', 'esa', 'demás', 'mismas', 'menudo', 'atras', 'veces', 'éste', 'aunque', 'siempre', 'cuánta', 'trabaja', 'pais', 'dicen', 'empleas', 'con', 'además', 'ésa', 'usais', 'pocos', 'luego', 'enseguida', 'aseguró', 'poca', 'proximo', 'será', 'vuestro', 'ello', 'bien', 'trata', 'ademas', 'ya', 'hizo', 'fuera', 'manera', 'ser', 'varios', 'embargo', 'aquel', 'durante', 'seis', 'propio', 'paìs', 'gran', 'todas', 'podeis', 'todavía', 'hay', 'dias', 'llegó', 'han', 'más', 'qeu', 'son', 'partir', 'para', 'conocer', 'dieron', 'poner', 'tenido', 'podrian', 'sois', 'esos', 'debido', 'por', 'antaño', 'estamos', 'sin', 'ese', 'cerca', 'nos', 'realizado', 'tus', 'propias', 'dia', 'emplear', 'cual', 'arribaabajo', 'consigue', 'pasado', 'primer', 'tenía', 'hecho', 'salvo', 'cuenta', 'habla', 'suya', 'cierto', 'eran', 'contra', 'mis', 'cualquier', 'segun', 'hubo', 'sobre', 'dijo', 'nuevas', 'somos', 'raras', 'algunos', 'después', 'pero', 'eras', 'enfrente', 'quiza', 'cierta', 'intento', 'cuanta', 'mio', 'sería', 'mios', 'tuyo', 'quizas', 'tendrá', 'misma', 'primero', 'tienen', 'mío', 'segundo', 'intentar', 'medio', 'sea', 'día', 'varias', 'hasta', 'ninguno', 'ante', 'aquél', 'considera', 'nuestra', 'encima', 'deprisa', 'habían', 'próximo', 'muchas', 'va', 'este', 'mencionó', 'un', 'contigo', 'trabajo', 'donde', 'lo', 'segunda', 'decir', 'era', 'haya', 'ningunas', 'creo', 'total', 'eramos', 'estados', 'través', 'estará', 'aquella', 'sabes', 'verdad', 'supuesto', 'intentamos', 'valor', 'tras', 'las', 'los', 'última', 'sólo', 'excepto', 'tuvo', 'lejos', 'próximos', 'estuvo', 'cuatro', 'estas', 'él', 'tanto', 'cada', 'yo', 'ésas', 'ésta', 'dejó']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stop Words de es_core_news_sm\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "stopwords_spacy = list(STOP_WORDS)\n",
    "print(stopwords_spacy)\n",
    "len(stopwords_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'todo', 'seáis', 'estada', 'estoy', 'se', 'habrás', 'hubierais', 'hubiésemos', 'esta', 'le', 'habidos', 'estuviesen', 'has', 'suyas', 'ha', 'tú', 'tuviese', 'como', 'eres', 'tenga', 'habíais', 'hubiese', 'sentida', 'fuerais', 'es', 'seré', 'mía', 'tengan', 'me', 'nosotros', 'quienes', 'mi', 'otra', 'estarán', 'hubieseis', 'cuando', 'fueran', 'seamos', 'vosotras', 'o', 'tuya', 'eso', 'fuésemos', 'en', 'siente', 'tendrían', 'hayan', 'el', 'tendrás', 'fui', 'tiene', 'habiendo', 'del', 'habríais', 'estar', 'vuestras', 'nuestro', 'serías', 'estos', 'hayáis', 'seremos', 'tenemos', 'ellas', 'tuvierais', 'teníamos', 'mías', 'una', 'nada', 'habré', 'tu', 'sus', 'hube', 'no', 'estuvieron', 'y', 'estabas', 'fueses', 'sean', 'tendríamos', 'vuestra', 'esto', 'habrán', 'tengo', 'poco', 'tenidas', 'estadas', 'tenida', 'estuve', 'otro', 'míos', 'tuyas', 'al', 'otras', 'muchos', 'estarían', 'estéis', 'sí', 'estuvieras', 'te', 'tuvieran', 'estaremos', 'algo', 'nosotras', 'habéis', 'seríais', 'suyo', 'hubiéramos', 'tuviera', 'habrá', 'estaréis', 'de', 'había', 'tendríais', 'habías', 'hubiesen', 'otros', 'hubiste', 'ti', 'sentido', 'fuimos', 'tuyos', 'porque', 'tengáis', 'qué', 'fuese', 'la', 'su', 'nuestras', 'están', 'tuvimos', 'estuviésemos', 'éramos', 'muy', 'mucho', 'vosotros', 'algunas', 'he', 'sentidos', 'estuviese', 'ella', 'fuiste', 'estarás', 'fuéramos', 'uno', 'tenían', 'fueseis', 'ni', 'suyos', 'estaría', 'nuestros', 'estén', 'os', 'tengamos', 'estarías', 'les', 'estábamos', 'también', 'e', 'tendrán', 'está', 'estés', 'tuviésemos', 'sentidas', 'antes', 'mí', 'estemos', 'estaríais', 'soy', 'estuvisteis', 'quien', 'tuvieses', 'estáis', 'que', 'fueron', 'esas', 'hubimos', 'tuvieras', 'entre', 'tuviste', 'unos', 'esté', 'serán', 'habida', 'estaban', 'hayamos', 'fue', 'todos', 'estaríamos', 'serás', 'ellos', 'estado', 'tenías', 'estando', 'estaba', 'estuviéramos', 'vuestros', 'habrías', 'hemos', 'fuisteis', 'tenidos', 'desde', 'esa', 'habría', 'tenéis', 'estuvierais', 'tendré', 'tendrías', 'hayas', 'hubieron', 'sintiendo', 'con', 'habréis', 'habido', 'tendremos', 'estad', 'teniendo', 'vuestro', 'será', 'ya', 'fuera', 'durante', 'hay', 'hubieses', 'han', 'más', 'estuviera', 'estuvieses', 'habíamos', 'son', 'para', 'tenido', 'tuvieron', 'sois', 'esos', 'por', 'estamos', 'sin', 'ese', 'nos', 'tus', 'tuvieseis', 'cual', 'hubisteis', 'habríamos', 'habrían', 'tenía', 'suya', 'eran', 'contra', 'mis', 'habremos', 'estuviste', 'a', 'hubieras', 'estuvieran', 'hubo', 'sentid', 'sobre', 'seas', 'somos', 'algunos', 'pero', 'eras', 'erais', 'tuve', 'hubiera', 'sería', 'hubieran', 'tened', 'tuyo', 'fuesen', 'tendrá', 'tienen', 'seréis', 'mío', 'sea', 'hasta', 'teníais', 'ante', 'tendría', 'nuestra', 'tuviesen', 'habían', 'habidas', 'este', 'un', 'serían', 'donde', 'lo', 'era', 'fueras', 'haya', 'estados', 'seríamos', 'tienes', 'estará', 'estabais', 'estuvieseis', 'estaré', 'las', 'los', 'tuviéramos', 'estás', 'tuvo', 'tendréis', 'tengas', 'estuvo', 'estas', 'él', 'tanto', 'estuvimos', 'tuvisteis', 'yo'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stop Words de nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_sp = set(stopwords.words('spanish'))\n",
    "print(stop_words_sp)\n",
    "len(stop_words_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perro\n",
      "frase\n",
      ".\n",
      "Y\n",
      "Naranja\n",
      ".\n",
      "frase\n",
      "tercer\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.is_stop == False:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('corro correr corriendo corredor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corro correr\n",
      "correr correr\n",
      "corriendo correr\n",
      "corredor corredor\n"
     ]
    }
   ],
   "source": [
    "for lem in doc:\n",
    "    print(lem.text, lem.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS (Part of Speech Tagging)\n",
    "\n",
    "Pueden ver el significado de las etiquetas en https://spacy.io/usage/linguistic-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('En tu final todo es bueno!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En ADP\n",
      "tu DET\n",
      "final NOUN\n",
      "todo PRON\n",
      "es AUX\n",
      "bueno ADJ\n",
      "! PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"es\" id=\"2366301c7833493487cdcf33b77d5d9b-0\" class=\"displacy\" width=\"1100\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">En</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">tu</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">final</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">todo</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">es</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">bueno!</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2366301c7833493487cdcf33b77d5d9b-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,89.5 395.0,89.5 395.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2366301c7833493487cdcf33b77d5d9b-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2366301c7833493487cdcf33b77d5d9b-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2366301c7833493487cdcf33b77d5d9b-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2366301c7833493487cdcf33b77d5d9b-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,2.0 925.0,2.0 925.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2366301c7833493487cdcf33b77d5d9b-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2366301c7833493487cdcf33b77d5d9b-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,89.5 920.0,89.5 920.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2366301c7833493487cdcf33b77d5d9b-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2366301c7833493487cdcf33b77d5d9b-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2366301c7833493487cdcf33b77d5d9b-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cop</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,266.5 L762,254.5 778,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección de identidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"El censo nacional de población y vivienda es la operación estadística de mayor envergadura y relevancia que una institución oficial de estadística pueda llevar a cabo, de allí que el actual director del Departamento Administrativo Nacional de Estadística-DANE, Dr. Juan Daniel Oviedo, haya conformado un comité de expertos en censos y demografía para evaluar el proceso y las cifras censales producidas en el Censo Nacional de Población y Vivienda para Colombia en 2018. Este comité está conformado por expertos nacionales e internacionales afiliados a instituciones académicas, consultores independientes, expertos del CELADE-División de Población de la CEPAL, UNFPA- LACRO, UNFPA Oficina de Colombia y de la Oficina de Colombia del Banco Mundial. El presente Resumen Ejecutivo destaca los principales resultados de esta evaluación realizada desde noviembre 1 de 2018 hasta junio 30 de 2019. Durante este tiempo, se mantuvo una conversación directa entre funcionarios del DANE y el comité, y en el último mes la institución compartió varias de las cifras básicas de las bases de información conformadas en la manufactura del Censo Nacional de Población y Vivienda para Colombia en 2018-CNPV. Como miembros del Comité de expertos agradecemos la generosidad del director y de todos los técnicos del DANE por compartir sus experiencias y conocimiento al respecto y sobre todo por abrir las puertas de la institución a nosotros en calidad de expertos para debatir sus resultados, de tanto interés para el país. Este escrito sigue el mismo orden de presentación de temas del informe final y cada párrafo termina con la sugerencia del Comité.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "El censo nacional de población y vivienda es la operación estadística de mayor envergadura y relevancia que una institución oficial de estadística pueda llevar a cabo, de allí que el actual director del Departamento Administrativo Nacional de Estadística-DANE, Dr. Juan Daniel Oviedo, haya conformado un comité de expertos en censos y demografía para evaluar el proceso y las cifras censales producidas en el Censo Nacional de Población y Vivienda para Colombia en 2018. Este comité está conformado por expertos nacionales e internacionales afiliados a instituciones académicas, consultores independientes, expertos del CELADE-División de Población de la CEPAL, UNFPA- LACRO, UNFPA Oficina de Colombia y de la Oficina de Colombia del Banco Mundial. El presente Resumen Ejecutivo destaca los principales resultados de esta evaluación realizada desde noviembre 1 de 2018 hasta junio 30 de 2019. Durante este tiempo, se mantuvo una conversación directa entre funcionarios del DANE y el comité, y en el último mes la institución compartió varias de las cifras básicas de las bases de información conformadas en la manufactura del Censo Nacional de Población y Vivienda para Colombia en 2018-CNPV. Como miembros del Comité de expertos agradecemos la generosidad del director y de todos los técnicos del DANE por compartir sus experiencias y conocimiento al respecto y sobre todo por abrir las puertas de la institución a nosotros en calidad de expertos para debatir sus resultados, de tanto interés para el país. Este escrito sigue el mismo orden de presentación de temas del informe final y cada párrafo termina con la sugerencia del Comité."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">El censo nacional de población y vivienda es la operación estadística de mayor envergadura y relevancia que una institución oficial de estadística pueda llevar a cabo, de allí que el actual director del \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Departamento Administrativo Nacional de Estadística-DANE\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Dr. Juan Daniel Oviedo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ", haya conformado un comité de expertos en censos y demografía para evaluar el proceso y las cifras censales producidas en el \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Censo Nacional de Población y Vivienda para Colombia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " en 2018. \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Este comité está\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " conformado por expertos nacionales e internacionales afiliados a instituciones académicas, consultores independientes, expertos del \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CELADE-División de Población de la CEPAL\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    UNFPA- LACRO\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    UNFPA Oficina de Colombia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " y de la \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Oficina de Colombia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " del \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Banco Mundial\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    El presente Resumen Ejecutivo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " destaca los principales resultados de esta evaluación realizada desde noviembre 1 de 2018 hasta junio 30 de 2019. \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Durante este tiempo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       ", se mantuvo una conversación directa entre funcionarios del \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    DANE\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " y el comité, y en el último mes la institución compartió varias de las cifras básicas de las bases de información conformadas en la manufactura del \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Censo Nacional de Población y Vivienda para Colombia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " en 2018-CNPV. \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Como miembros del Comité de expertos agradecemos la generosidad del director\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " y de todos los técnicos del \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    DANE\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " por compartir sus experiencias y conocimiento al respecto y sobre todo por abrir las puertas de la institución a nosotros en calidad de expertos para debatir sus resultados, de tanto interés para el país. \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Este escrito\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " sigue el mismo orden de presentación de temas del informe final y cada párrafo termina con la sugerencia del \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Comité\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style = 'ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar las librerías necesarias para leer los archivos de TASS\n",
    "import xmltodict\n",
    "import json\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traer el archivo xml original xml y convertirlo en un diccionario, escogimos el de Costa rica\n",
    "with open(\"TASS2019_country_CR_train.xml\") as xml_file:\n",
    "    data_dict = xmltodict.parse(xml_file.read())\n",
    "xml_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertir a json el diccionario\n",
    "json_data = json.dumps(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escribir en un archivo el resultado en json\n",
    "with open(\"TASS2019_country_CR_train.json\", \"w\") as json_file:\n",
    "    json_file.write(json_data)\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpieza de datos en la fuente\n",
    "#Original en json\n",
    "fin = open(\"TASS2019_country_CR_train.json\", \"rt\")\n",
    "#Archivo resultante en json\n",
    "fout = open(\"TASS2019_country_CR_train-sintilde.json\", \"wt\")\n",
    "#Procesamiento de lìneas del archivo\n",
    "for line in fin:\n",
    "\t#Reemplazar los caracteres unicode, no se dejaron tildes porque causan error\n",
    "    strtmp1 = line.replace('\\\\u00f1', 'ñ')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00e1', 'a')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00e9', 'e')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00ed', 'i')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00f3', 'o')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00fa', 'u')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00bf', '¿')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00a1', '¡')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00d1', 'Ñ')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00c1', 'A')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00c9', 'E')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00cd', 'I')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00d3', 'O')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00da', 'U')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00fc', 'ü')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00b0', '')\n",
    "    #Quitar el inicio y el fin del json para dejar solo los tweets\n",
    "    strtmp1 = strtmp1.replace('{\"tweets\": {\"tweet\": ', '')\n",
    "    strtmp1 = strtmp1.replace(']}}', ']')\n",
    "    #Quitar el diccionario que contiene la polaridad y dejarla solo con su valor de sentimiento\n",
    "    strtmp1 = strtmp1.replace('\"sentiment\": {\"polarity\": {\"value\": ', '\"sentiment\": ')\n",
    "    strtmp1 = strtmp1.replace('\"NONE\"}}', '\"NONE\"')\n",
    "    #Asignamos al sentimiento positivo el valor de 1\n",
    "    strtmp1 = strtmp1.replace('\"P\"}}', '1')\n",
    "    strtmp1 = strtmp1.replace('\"NEU\"}}', '\"NEU\"')\n",
    "    #Asignamos al sentimiento negativo el valor de 0\n",
    "    strtmp1 = strtmp1.replace('\"N\"}}', '0')\n",
    "    #eliminación de puntuaciones\n",
    "    strtmp1 = re.sub('[¡!#$).;¿?&°]', '', strtmp1.lower())\n",
    "    fout.write(strtmp1)\n",
    "#cerrar archivos\n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@noilymv yo soy totalmente puntual</td>\n",
       "      <td>2016-08-23 23:16:23</td>\n",
       "      <td>es</td>\n",
       "      <td>none</td>\n",
       "      <td>768225400254111744</td>\n",
       "      <td>14628107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@sandracauffman hola sandrita no le habia dese...</td>\n",
       "      <td>2016-08-29 01:54:14</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td>770077064833671168</td>\n",
       "      <td>713600676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>si andan haciendo eso mejor se quedaran callad...</td>\n",
       "      <td>2016-09-01 04:46:19</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>771207534342320128</td>\n",
       "      <td>120940293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>que pereza quiero choco banano</td>\n",
       "      <td>2016-09-03 02:40:58</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>771900763987513344</td>\n",
       "      <td>2827444381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@robertobrenes bueno, no es tanto lo mayor com...</td>\n",
       "      <td>2016-09-04 21:43:01</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>772550560998301696</td>\n",
       "      <td>60878511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content                date lang  \\\n",
       "0                 @noilymv yo soy totalmente puntual 2016-08-23 23:16:23   es   \n",
       "1  @sandracauffman hola sandrita no le habia dese... 2016-08-29 01:54:14   es   \n",
       "2  si andan haciendo eso mejor se quedaran callad... 2016-09-01 04:46:19   es   \n",
       "3                     que pereza quiero choco banano 2016-09-03 02:40:58   es   \n",
       "4  @robertobrenes bueno, no es tanto lo mayor com... 2016-09-04 21:43:01   es   \n",
       "\n",
       "  sentiment             tweetid        user  \n",
       "0      none  768225400254111744    14628107  \n",
       "1         1  770077064833671168   713600676  \n",
       "2         0  771207534342320128   120940293  \n",
       "3         0  771900763987513344  2827444381  \n",
       "4         0  772550560998301696    60878511  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tomar los datos del archivo creado en un dataframe\n",
    "train_df = pd.read_json('TASS2019_country_CR_train-sintilde.json')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para eliminar las menciones a otros usuarios de twitter\n",
    "def filter_reply(content):\n",
    "    temp = content\n",
    "    while temp.find(\"@\") > -1:\n",
    "        temp = temp[:temp.find(\"@\")] + temp[(temp.find(\" \",temp.find(\"@\"))):]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hola sandrita no le habia deseado un feliz di...</td>\n",
       "      <td>2016-08-29 01:54:14</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td>770077064833671168</td>\n",
       "      <td>713600676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>si andan haciendo eso mejor se quedaran callad...</td>\n",
       "      <td>2016-09-01 04:46:19</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>771207534342320128</td>\n",
       "      <td>120940293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>que pereza quiero choco banano</td>\n",
       "      <td>2016-09-03 02:40:58</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>771900763987513344</td>\n",
       "      <td>2827444381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bueno, no es tanto lo mayor como cuanto de ca...</td>\n",
       "      <td>2016-09-04 21:43:01</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>772550560998301696</td>\n",
       "      <td>60878511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>el de halfon de germinal se ve mortal y los d...</td>\n",
       "      <td>2016-09-04 02:45:38</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td>772264329433509888</td>\n",
       "      <td>373097882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content                date lang  \\\n",
       "1   hola sandrita no le habia deseado un feliz di... 2016-08-29 01:54:14   es   \n",
       "2  si andan haciendo eso mejor se quedaran callad... 2016-09-01 04:46:19   es   \n",
       "3                     que pereza quiero choco banano 2016-09-03 02:40:58   es   \n",
       "4   bueno, no es tanto lo mayor como cuanto de ca... 2016-09-04 21:43:01   es   \n",
       "6   el de halfon de germinal se ve mortal y los d... 2016-09-04 02:45:38   es   \n",
       "\n",
       "  sentiment             tweetid        user  \n",
       "1         1  770077064833671168   713600676  \n",
       "2         0  771207534342320128   120940293  \n",
       "3         0  771900763987513344  2827444381  \n",
       "4         0  772550560998301696    60878511  \n",
       "6         1  772264329433509888   373097882  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quitar menciones del texto\n",
    "train_df['content'] = train_df['content'].apply(filter_reply)\n",
    "#Quitar columnas sin clasificación de sentimiento \n",
    "indexNames = train_df[(train_df['sentiment'] == 'none') | (train_df['sentiment'] == 'neu')].index\n",
    "train_df.drop(indexNames , inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerías de aprendizaje\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    310\n",
       "1    221\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificar frecuencias de cada categoría\n",
    "train_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content      0\n",
       "date         0\n",
       "lang         0\n",
       "sentiment    0\n",
       "tweetid      0\n",
       "user         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificar si hay datos nulos\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~¡¿'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Constante de signos de puntuación (para referencia pues se eliminaron en el archivo fuente)\n",
    "import string\n",
    "puntua = string.punctuation + '¡¿'\n",
    "puntua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para limpieza de datos\n",
    "def text_data_cleaning(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.lemma_ != \"-PRON-\":\n",
    "            temp = token.lemma_.strip()\n",
    "        else:\n",
    "            temp = token\n",
    "        tokens.append(temp)\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords_spacy and token not in puntua:\n",
    "            clean_tokens.append(token)\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola', 'Te', 'gustar', 'meetup']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data_cleaning(\"¡Hola cómo estás!. ¿Te gusta el meetup?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization Feature Engineering (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar librería de vectorización\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir la función de tokenizado y crear el clasificador\n",
    "tfidf = TfidfVectorizer(tokenizer = text_data_cleaning)\n",
    "classifier = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear los vectores de datos\n",
    "X = train_df['content']\n",
    "y = train_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((424,), (107,), (424,), (107,))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Crear el vector de entrenamiento como una porción de los datos y dejar el resto para pruebas\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595    nunca dejen el snapchat abierto en el celular ...\n",
       "771     no se si a todos les funcione pero nosotros s...\n",
       "255    apoyen a la network   esta network tiene futur...\n",
       "533                 pues es un buen momento para hacerlo\n",
       "98      y son 3030 años de acompañarnos,30 es mi edad...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear un pipeline\n",
    "clf = Pipeline([('tfidf', tfidf), ('clf', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evitar que el formato se tome como unknown\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entrenar el clasificador\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear el vectos de valores predichos a partir del clasificador\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.84      0.83        68\n",
      "           1       0.70      0.67      0.68        39\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       107\n",
      "   macro avg       0.76      0.75      0.76       107\n",
      "weighted avg       0.77      0.78      0.77       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ver la precisión obtenida\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[57, 11],\n",
       "       [13, 26]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Crear la matriz de confusión\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predecir algunas frases de prueba\n",
    "clf.predict(['Realmente me gustó mucho este ejercicio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Una negativa\n",
    "clf.predict(['La verdad esto apesta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#El sarcasmo en twitter es difícil de predecir\n",
    "clf.predict(['Gracias por su interés señor, sea tan amable largarse y que tenga un buen día'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Probemos con un tweet sin sarcasmo\n",
    "clf.predict(['Esta gente estaría buena para escribir novelas de ficción. Desgraciados. Me siento indignada.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#¿Qué dice el clasificador del Meetup de hoy?\n",
    "clf.predict(['Vale la pena haber asistido'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
